--- 
title: "iDigBio Results in National Parks"
output:
  html_document:
    fig_width: 8
    fig_height: 6
  pdf_document:
    fig_width: 8
    fig_height: 6
--- 

```{r, load_packages, message=FALSE, echo=FALSE, warning=FALSE}
library(ggmap)
library(jsonlite)
library(tidyverse)
require(sp)
library(rgdal)
library(ridigbio)
library(knitr)

IDIGBIO_API_RESULTS_FILE = "./data/idigbio_api_results.json"
RECORDS_WITHIN_BOUNDS_OUTPUT = "./output/records_within_bounds.tsv"
USE_RANDOM_SUBSET = TRUE
RANDOM_SUBSET_AMOUNT = 10000
```

The general steps we want to take:

1. Get data from the iDigBio API
1. Get shape data for parks from the National Park Service (NPS)
1. From the API results, find the records that are inside of parks

Lets get data from the API.  Here, I'll search for stateprovince = "Florida"
```{r,get_idigbio_data,  cache=TRUE, message=FALSE, echo=FALSE, warning=FALSE}
####################
# Step: get data from iDigBio
# a tibble is essential a data frame, slightly modernized by the tidyverse
idigbio_api_results = tibble()

if (!file.exists(IDIGBIO_API_RESULTS_FILE)) {
  idigbio_api_results = as_tibble(idig_search_records(rq = list(
      stateprovince = "Florida"
    ),
    fields = c("uuid", "etag", "geopoint", "recordset", "stateprovince", 
               "institutioncode", "phylum", "basisofrecord", "kingdom", 
               "data.dwc:coordinateUncertaintyInMeters", "institutionid", 
               "collectioncode", "country", "county", "catalognumber"),
    limit = 100000)
  )
  write_json(idigbio_api_results, IDIGBIO_API_RESULTS_FILE)
  
} else {
  # It is critical to include simplifyVector = TRUE or else it reads back as a list.
  idigbio_api_results = as_tibble(read_json(IDIGBIO_API_RESULTS_FILE, simplifyVector = TRUE))
}

if(USE_RANDOM_SUBSET){
  set.seed(123456789)
  idigbio_api_results = sample_n(idigbio_api_results, RANDOM_SUBSET_AMOUNT)
}
```

Here's a small look at the data we get back:
```{r, show_initial_results, message=FALSE, echo=FALSE, warning=FALSE}

head(idigbio_api_results)

```

```{r, clean_data, message=FALSE, echo=FALSE, warning=FALSE}
# clean our data. Note that backticks are important for picking up proper column
# we also could have used API fields to insist on records that have geopoints
api_results_clean = idigbio_api_results %>% 
  filter(!is.na(`geopoint.lon`)) %>% 
  filter(!is.na(`geopoint.lat`)) %>% 
  mutate(lon = as.numeric (`geopoint.lon`)) %>% 
  mutate(lat = as.numeric (`geopoint.lat`)) %>% 
  rownames_to_column(var = "id")
```  

```{r, get_shape_data, include=FALSE, message=FALSE, echo=FALSE, warning=FALSE}
########################################
# Step: Read in the shape data
# 
#  rgdal requires gdal, which might lead to frustrations getting it installed/compiled
# note the lack of file extension on "nps_boundary" in layer
nps_shapes = readOGR(dsn="nps_boundary/Current_Shapes/Data_Store/06-06-12_Posting", layer="nps_boundary")

# roughly half of the entire national list doesn't have a state value specified, which makes it
# difficult to restrict based on that field.
# We'll shortcut this for now with finding the codes manually.
# Other options include perhaps seeing if the NPS API (free bu requires signup) allows us to do so
# 
# Another option would be to do the opposite lookup and restrict the parks to only parks that have 
# point matches inside of them.
park_codes_of_interest = c("BICY", "BISC", "CANA", "CASA", "DESO", "DRTO", "EVER", "FOCA", "FOMA", "GUIS", "GUGE", "TIMU")

# dplyr filter will not work here with S4 object (TODO), so we'll get a logical vector of matches first.
matches_vector = nps_shapes@data$UNIT_CODE %in% park_codes_of_interest
nps_shapes_in_fl = nps_shapes[matches_vector,]

########################################
# Step: create a SpatialPointsDataFrame that we can use to search
# 
# data.frame() creates a new dataframe based on the df argument
api_results_spdf = data.frame(api_results_clean)

# coordinates() takes a dataframe and creates a SpatialPointsDataFrame
coordinates(api_results_spdf) = c("lon", "lat")

# In order to be mapped together, points and shapes need use the same projection system.
# we've checked that they're the same projection system by viewing:
# nps_shapes@proj4string
# which shows:
# CRS arguments:
#  +proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs 
# 
# and so we know it's "safe" to assign it
#
proj4string(api_results_spdf) <- proj4string(nps_shapes)
```

Here are the names and "codes" of the parks in Florida:
* https://www.nps.gov/state/fl/index.htm
```{r, show_park_names, echo=FALSE}

nps_shapes_in_fl@data %>% 
  arrange(UNIT_CODE) %>% 
  select(UNIT_CODE, UNIT_NAME)

```

```{r, perform_intersection, message=FALSE, echo=FALSE, warning=FALSE}
########################################
# Step: perform intersection search

# compute a df full of true/false for whether it intersects with our areas of interest
intersections <- over(api_results_spdf, nps_shapes_in_fl[,"UNIT_CODE"]) 

# assign those true/false values back into our data.
# "UNIT_CODE" is from the NPS data and represents the park abbreviation
# any result with a park code set will be an overlap
# any result without a park code in park_code is not within the bounds
api_results_clean$park_code = intersections$UNIT_CODE

# now filter out points that don't fall within the boundaries
points_that_overlap = api_results_clean %>% 
  filter(! is.na(park_code))

# ggmap needs a data frame, so we're now going to change it back.
nps_shapes_that_intersect_df = fortify(nps_shapes_in_fl)

# due to some quirkiness with handling automatically generated IDs in R and the NPS shape data,
# we'll build this to allow us to map an ID to a park code (UNIT_CODE)
park_id_lookup = nps_shapes_in_fl@data %>% 
  rownames_to_column(var = "park_id") %>% 
  select(park_id, UNIT_CODE) 

# add our park UNIT_CODE into our dataframe via a "join"-type behavior  
nps_shapes_that_intersect_df <- merge(nps_shapes_that_intersect_df, park_id_lookup, by.x = "id", by.y = "park_id", all.x = TRUE)
```

```{r, mapping_and_outputs, message=FALSE, echo=FALSE, warning=FALSE}
########################################
# Step: set up mapping

# these will add some extra room along the edges of our map
negative_margin = -1
positive_margin = 1

bounding_box = c("left" = min(points_that_overlap$lon) + negative_margin, 
                 "bottom" = min(points_that_overlap$lat) + negative_margin, 
                 "right"=max(points_that_overlap$lon) + positive_margin, 
                 "top" = max(points_that_overlap$lat) + positive_margin
  )
# if you try using get_map(),  you might see this: 
# "Error: Google now requires an API key. See ?register_google for details."
# 
# So we'll use a stamen map below.  Note that this requires us to specify the bounding box
m = get_stamenmap(bbox = bounding_box, zoom = 7, maptype = "toner-lite", source = "stamen" )

########################################
# OUTPUTS

write_tsv(points_that_overlap, RECORDS_WITHIN_BOUNDS_OUTPUT)
```

```{r, rename_unit_code, echo=FALSE, message=FALSE,warning=FALSE}
# rename column here to make more sense in our graphs
nps_shapes_that_intersect_df = nps_shapes_that_intersect_df %>% 
  rename(park_code = UNIT_CODE)
```

```{r, draw_plots, cache=TRUE, message=FALSE, echo=FALSE, warning=FALSE}
########################################
# map 1
# a plot of all search result records with geopoints before doing the intersection search

# note that with both maps, we're using the kingdom API field for plotting points, and so any points
# that have an NA for that field will not appear on the map (but will be present in the output data)

map_1_subtitle = paste("Total records with geopoints:", api_results_clean %>% summarise(n = n()))



ggmap(m,
      base_layer = ggplot(api_results_clean, aes(x = lon, y = lat)) + 
      ggtitle("All points from search results", subtitle = map_1_subtitle) 
      ) + 
  geom_polygon(aes(x = long, y = lat, group = group, fill = park_code), 
               data = nps_shapes_that_intersect_df, alpha = 0.5) + 
  geom_point(data = api_results_clean, aes(shape = kingdom, color = kingdom), alpha = 0.7)



########################################
# map 2
# a plot of all search result records within the boundaries of interest
map_2_subtitle = paste("Total records in bounds:",  
                       points_that_overlap %>% summarise(n = n()),
                       " / ",
                       api_results_clean %>% summarise(n = n()))

ggmap(m,
      base_layer = ggplot(points_that_overlap, aes(x = lon, y = lat, alpha = 0.7))+ 
        ggtitle("Search results within park bounds", subtitle = map_2_subtitle)
) + 
  geom_polygon(aes(x = long, y = lat, group = group, fill = park_code), 
               data = nps_shapes_that_intersect_df, alpha = 0.5) + 
  geom_point(data = points_that_overlap, aes(shape = kingdom, color = kingdom), alpha = 0.7)

########################################
# plot 1: counts by park

plot_1_title = paste("counts by park", "( n =", points_that_overlap %>% summarize(n = n()), ")")

ggplot(points_that_overlap, aes(park_code)) +
  ggtitle(plot_1_title) + 
  geom_bar(aes(fill = kingdom))

plot_2_title = paste("counts by institution code", "( n =", points_that_overlap %>% summarize(n = n()), ")")


# plot 2: counts by institutioncode
ggplot(points_that_overlap, aes(institutioncode)) +
  ggtitle(plot_2_title) +
  geom_bar(aes(fill = kingdom)) +
  coord_flip()


```

```{r, include=FALSE}
plot_3_title = paste("counts by basis of record", "( n =", points_that_overlap %>% summarize(n = n()), ")")

# plot 3: counts by basisofrecord
ggplot(points_that_overlap, aes(basisofrecord)) +
  ggtitle(plot_2_title) +
  geom_bar(aes(fill = kingdom)) +
  coord_flip()
```